docker环境搭建
--------------------
	安装虚拟机
	安装docker
	配置加速器
	配置docker-compose
	·····

在docker中搭建Hadoop集群
--------------------------------------
	1. 拉取镜像
		$>sudo docker pull kiwenlau/hadoop:1.0
	2. 克隆github仓库
		$>git clone https://github.com/kiwenlau/hadoop-cluster-docker
	3. 创建Hadoop网络
		$>docker network create --driver=bridge hadoop
	4. 运行Docker容器
		$>cd hadoop-cluster-docker
		$>./start-container.sh
		运行结果：
		start hadoop-master container...
		start hadoop-slave1 container...
		start hadoop-slave2 container...
		root@hadoop-master:~#
		启动了3个容器，1个master, 2个slave
		运行后就进入了hadoop-master容器的/root目录
	5. 启动Hadoop
		$>./start-hadoop.sh
	Hadoop集群搭建完成
	注意：此操作容器是使用start-container.sh文件创建的，并非yml
	提示：此容器基础镜像为Ubuntu，进行vi操作时，方向键可能无效，若出现此情况，则：
		$>cp /etc/vim/vimrc ~/.vimrc 

配置zookeeper
--------------------------------
	1. 下载文件zookeeper-3.4.6.tar.gz，并将其上传到宿主机，
		建议在宿主机中配置好，然后统一上传到容器中
		以下操作无特别说明均在宿主机中完成
	2. 解压缩
		$>tar -zxvf zookeeper-3.4.6.tar.gz -C /usr/local
		$>chmod 755 zookeeper-3.4.6
	3. 创建数据目录及日志目录
		$> cd /usr/local/zookeeper-3.4.6
		$> mkdir data
		$> mkdir logs
	4. 配置zoo.cfg
		$> cd /usr/local/zookeeper-3.4.6/conf
		$> cp zoo_sample.cfg zoo.cfg
		$> vi zoo.cfg
		[zoo.cfg] 添加如下配置
			dataDir=/usr/local/zookeeper-3.4.6/data
			dataLogDir=/usr/local/zookeeper-3.4.6/logs 
			server.1=hadoop-master:2888:3888
			server.2=hadoop-slave1:2888:3888
			server.3=hadoop-slave2:2888:3888
	5. 将宿主机上的zookeeper包分发给到各容器中
		$>cd /usr/local
		$>docker cp zookeeper-3.4.6 hadoop-master:/usr/local
		$>docker cp zookeeper-3.4.6 hadoop-slave1:/usr/local
		$>docker cp zookeeper-3.4.6 hadoop-slave2:/usr/local
	7. 修改各容器的/data/myid
		$> vi /usr/local/zookeeper-3.4.6/data/myid
			[hadoop-master]: 1
			[hadoop-slave1]: 2
			[hadoop-slave2]: 3
		注：hadoop-master的myid写 1，hadoop-slave1的myid写 2，hadoop-slave2的myid写 3 
	8. 在三个容器中都启动zookeeper
		$>cd /usr/local/zookeeper-3.4.6/bin
		$>./zkServer.sh start
		注：配置环境变量之后，直接执行zkServer.sh start即可
			配置环境变量在/etc/profile或者/root/.bashrc都可以，区别略
		环境变量配置：
			$>vi /root/.bashrc
				#zookeeper
				export ZOOKEEPER_HOME=/usr/local/zookeeper-3.4.6
				export PATH=$PATH:$ZOOKEEPER_HOME/bin
			$>source /root/.bashrc
	9. 测试：
		各容器中均执行以下操作查看zookeeper状态
			$>zkServer.sh status
		应该是一台leader，两台follower
		杀死leader
			$>kill -9 [进程号]
		重新查看状态
		follower中有一台变为leader，说明配置成功
		
配置HA
---------------------------------
	1. 修改配置文件
		[core-site.xml]
		[hdfs-site.xml]
			注意journalnode服务器存储目录，没有要创建
		[yarn-site.xml]
		[mapred-site.xml]
		修改好后分发到各容器，
		若你是在master中修改的，分发使用scp命令；若是在宿主机中修改的，分发使用docker cp命令
	2. 启动zookeeper
		分别在每个节点上执行
		$>zkServer.sh start
		查看进程信息，结果如下
		========== hadoop-master ==========
		1630 Jps
		1502 QuorumPeerMain
		========== hadoop-slave1 ==========
		1388 Jps
		1243 QuorumPeerMain
		========== hadoop-slave2 ==========
		683 QuorumPeerMain
		756 Jps
	3. 格式化namenode，并执行同步操作
		[hadoop-master]  --> nn1
		$> hdfs namenode -format 
		[hadoop-slave1]  --> nn2
		// 同步nn2  *******
		$> hdfs namenode -bootstrapStandby
	4. hadoop-master格式化zkfc(以下操作无特别说明均在hadoop-master上进行)
		$hdfs zkfc -formatZK
	5. 启动集群测试
		$>start-dfs.sh
		查看进程信息，结果如下
		========== hadoop-master ==========
		136 QuorumPeerMain
		1985 DFSZKFailoverController
		2244 NameNode
		3351 Jps
		1809 JournalNode
		========== hadoop-slave1 ==========
		1189 DFSZKFailoverController
		1055 JournalNode
		128 QuorumPeerMain
		976 DataNode
		2127 Jps
		1831 NameNode
		========== hadoop-slave2 ==========
		461 JournalNode
		273 QuorumPeerMain
		873 Jps
		382 DataNode
		 - 若显示以上进程，HA配置成功
		 - 若master和slave1均未启动namenode，则：
			查看日志/usr/local/hadoop/logs/hadoop-root-namenode-hadoop-master.log（日志看自己具体是哪一个）
			若出现多个JournalNode not formatted错误，则：
			$>hdfs namenode -initializeSharedEdits
		- 若master启动namenode，而slave1没有，则：
			查看salve1日志/usr/local/hadoop/logs/hadoop-root-namenode-hadoop-slave1.log
			若报错no valid image files found，说明同步操作失败，可直接拷贝master的/root/hdfs/namenode/current文件到slave1
		重新启动集群查看
	6. 查看namenode节点状态
		$>hdfs haadmin -getServiceState nn1
		$>hdfs haadmin -getServiceState nn2
		正常情况：一个状态为action，另一个为standby
		测试，杀死状态为action的namenode 
		$>kill -9 [进程号]
		查看另一个namenode的状态，若变为action，则配置成功
	7. 启动yarn集群测试
		$>start-yarn.sh
		然后在hadoop-slave1中执行：
		$>yarn-daemon.sh start resourcemanager
		查看进程信息
		========== hadoop-master ==========
		2695 JournalNode
		2281 QuorumPeerMain
		2474 NameNode
		3018 Jps
		2859 DFSZKFailoverController
		2940 ResourceManager
		========== hadoop-slave1 ==========
		1804 DFSZKFailoverController
		1614 DataNode
		1542 NameNode
		1977 Jps
		1706 JournalNode
		1439 QuorumPeerMain
		1910 NodeManager
		========== hadoop-slave2 ==========
		1151 QuorumPeerMain
		1406 NodeManager
		1297 JournalNode
		1514 Jps
		1213 DataNode
	8. 查看resourcemanager节点状态
		$>yarn rmadmin -getServiceState rm1
		$>yarn rmadmin -getServiceState rm2
		正常情况：一个状态为action，另一个为standby
		测试，杀死状态为action的resourcemanager 
		$>kill -9 [进程号]
		查看另一个resourcemanager的状态，若变为action，则配置成功

Scala安装
---------------------------------
	1. 解压文件
		$>tar -xvf scala-2.12.8.tgz
		并将解压的文件copy到各容器
	2. 每个容器都要配置系统环境变量
		$>vi /etc/profile
			export SCALA_HOME=/usr/local/scala-2.12.8
			export PATH=$PATH:$SCALA_HOME/bin
		$>source /etc/profile
	3. 测试
		$>scala
		scala> :quit

spark安装
----------------------------------
	1. 安装spark
		$>tar -xvf spark-2.4.3-bin-hadoop2.7.tgz
	2. 改名
		$> mv spark-2.4.3-bin-hadoop2.7 spark-2.4.3
		并拷贝到各容器
	3. 各容器配置系统环境变量
		$>vi /etc/profile
			export SPARK_HOME=/usr/local/spark-2.4.3
			export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin
	4. 测试
		$>spark-shell

spark配置
-------------------------------------
	1. 修改slaves文件内容
		$>cd /usr/local/spark-2.4.3/conf
		$>cp slaves.template slaves
			hadoop-master
			hadoop-slave1
			hadoop-slave2
		$>source /etc/profile		
	2. 修改spark-env.sh文件
		$>cp spark-env.sh.template spark-env.sh
			export JAVA_HOME=/usr/java/jdk1.8.0_91
			export HADOOP_CONF_DIR=/usr/local/hadoop-2.7.3/etc/hadoop
			export SCALA_HOME=/usr/local/scala-2.12.8
			export SPARK_MASTER_HOST=hadoop-master
			export SAPRk_WORKER_MEMORY=1g #每个worker进程能管理1g内存
	3. 重命名
		$>cd /usr/local/spark-2.4.3/sbin
		$>mv start-all.sh start-spark-all.sh
		$>mv stop-all.sh stop-spark-all.sh
	4. copy到slave节点
		$>xsync.sh /usr/local/scala-2.12.8
		$>xsync.sh /usr/local/spark-2.4.3
		$>xsync.sh /etc/profile
	5. slave节点环境变量立即生效
		$>source /etc/profile
	6. 启动spark
		start-spark-all.sh
	7. 查看节点信息
		$>xcall.sh jps
		========== hadoop-master ==========
		3297 Master
		3651 NameNode
		3396 Worker
		3413 Worker
		2695 JournalNode
		3752 Jps
		2281 QuorumPeerMain
		2859 DFSZKFailoverController
		2940 ResourceManager
		========== hadoop-slave1 ==========
		2303 Jps
		1804 DFSZKFailoverController
		1614 DataNode
		2156 Worker
		1542 NameNode
		1706 JournalNode
		1439 QuorumPeerMain
		1910 NodeManager
		========== hadoop-slave2 ==========
		1718 Jps
		1151 QuorumPeerMain
		1406 NodeManager
		1600 Worker
		1297 JournalNode
		1213 DataNode
	8. 关闭集群
		$>stop-spark-all.sh

	错误：
	WARN org.apache.hadoop.hdfs.server.namenode.ha.EditLogTailer: Edit log tailer interrupted 
	java.lang.InterruptedException: sleep interrupted
	解决方法：
	$> apt-get remove iptables

------------------------------------------------
使用dockerfile创建镜像，一键部署ssh + Hadoop HA + spark
	1. 编写Dockerfile
	2. 运行Dockerfile
		$> docker build -t hadoop-master:5.0 ./
		$> docker build -t hadoop-slave1:5.0 ./
		$> docker build -t hadoop-slave2:5.0 ./
	3. 启动容器haodoop-master hadoop-slave1 hadoop-slave2
		$> docker run -itd \
			--privileged -e "container=docker" \
			-v /sys/fs/cgroup:/sys/fs/cgroup \
			--net=hadoop \
			-p 50070:50070 \
			-p 8088:8088 \
			-p 8080:8080 \
			--name hadoop-master \
			--hostname hadoop-master \
			hadoop-master:5.0 \
			/usr/sbin/init
		$> docker run -itd \
			--privileged -e "container=docker" \
			-v /sys/fs/cgroup:/sys/fs/cgroup \
			--net=hadoop \
			--name hadoop-slave1 \
			--hostname hadoop-slave1 \
			hadoop-slave1:5.0 \
			/usr/sbin/init
		$> docker run -itd \
			--privileged -e "container=docker" \
			-v /sys/fs/cgroup:/sys/fs/cgroup \
			--net=hadoop \
			--name hadoop-slave2 \
			--hostname hadoop-slave2 \
			hadoop-slave2:5.0 \
			/usr/sbin/init
	4. 进入hadoop-master
		$> docker exec -ti hadoop-master /bin/bash
	5. 执行以下操作
		$> zkService.sh start
		$> hdfs zkfc -formatZK
		$> start-dfs.sh
		$> hdfs namenode -format               # HA环境下必须先启动start-dfs.sh才能格式化namenode
		$> hdfs namenode -bootstrapStandby
		$> scp -r /root/hdfs/namenode/current/ root@hadoop-slave1:/root/hdfs/namenode/
		$> stop-dfs.sh
	配置完成！

	
一键部署
------------------------------
	1. 编写Dockerfile
	2. 编写docker-compose.yml
	3. 运行Dockerfile
		$> docker build -t harper/hadoop-cluster:7.0 ./
	4. 运行docker-compose.yml
		$> docker-compose up -d

//	#5. 启动容器之后：
//		1. zkService.sh start
//		2. hdfs zkfc -formatZK
//		3. start-dfs.sh
//		4. hdfs namenode -format               #HA环境下必须先启动start-dfs.sh才能格式化namenode
//		5. scp -r /root/hdfs/namenode/current/ root@hadoop-slave1:/root/hdfs/namenode/
//		6. stop-dfs.sh 
//		7. start-dfs.sh
//		8. hdfs haadmin -getServiceState nn1
//		   hdfs haadmin -getServiceState nn2
		   ...
	5. 编写脚本，启动容器之后执行
		$>/file/start.sh
		一键启动hadoop ha集群 spark集群

上传任务至spark集群处理
-----------------------------
	1. hdfs dfs -mkdir /upload
	2. hdfs dfs -put test.txt /upload/
	3. spark-shell
	4. sc.textFile("hdfs://mycluster/upload/test.txt").flatMap(line => line.split(" ")).map(word => (word, 1)).reduceByKey(_ + _).sortBy(_._2,false).take(10).foreach(println)


//MySQL读写分离
//--------------------------------------
//	1. 安装mysql镜像
//		$> docker pull mysql
//	2. 启动mysql容器
//		$> docker run -d -e MYSQL_ROOT_PASSWORD=root --name mysql-test --hostname mysql-test mysql
//	3. 进入容器
//		$> docker exec -ti mysql-test /bin/bash
//	4. 安装vim编辑器
//		$> apt-get update
//		$> apt-get install vim
//	5. 退出，并提交新的tag
//		$> exit
//		$> docker commit -m 'mysql install vim' mysql-test mysql:init
//	6. 设置主机
//		$> docker run -d --name mysql-master --hostname mysql-test -v /root/mysql/master:/var/lib/mysql -p 3306:3306 -e MYSQL_ROOT_PASSWORD=root mysql
//	7. 编辑docker.cnf
//		$> docker exec -ti mysql-master /bin/bash
//		$> vi /etc/mysql/conf.d/docker.cnf
//			添加：
//			server-id=1   //给数据库服务的唯一标识，一般为大家设置服务器Ip的末尾号
//			log-bin=master-bin
//			log-bin-index=master-bin.index
//	8. 新建用户连接该机器mysql，创建一个用户‘repl’，并允许其他Slave服务器可以通过远程访问Master，通过该用户读取二进制日志，实现数据同步。
//		create user repl;
//		GRANT REPLICATION SLAVE ON *.* TO 'repl'@'172.17.0.%' IDENTIFIED BY 'mysql';

MySQL主从复制
--------------------------------------------
	1. 编写docker-compose.yml,启动mysql集群
	2. 进入mysql-master和mysql-slave
	3. 在master上执行
		mysql> GRANT REPLICATION SLAVE ON *.* to 'reader'@'%' identified by 'reader'; 
		mysql> FLUSH PRIVILEGES;
		mysql> show master status;
	4. 在slave上执行
		mysql> stop slave;  #首次创建不需要执行
		#注意用户名密码为第三步创建的
		#注意master_log_file和master_log_pos与第三步mysql-master上面展示的一致
		mysql> change master to master_host='172.17.0.7',master_user='reader',master_password='reader',master_log_file='mysql-bin.000003',master_log_pos=765;
		mysql> start slave;
		mysql> show slave status\G
		Slave_IO_Running: Yes，Slave_SQL_Running: Yes即表示启动成功。
	5. 测试
		在mysql-master上创建数据库slavetest
			mysql> create database slavetest;
			mysql> show databases;
		在mysql-slave上查看
			mysql> show databases;


Nginx+Tomcat实现负载均衡和反向代理
----------------------------------------------
	1. 编写Dockerfile生成tomcat基础镜像，主要增加jdk
	2. 创建index.jsp，用于测试负载均衡
		hello tomcat1/2/3
	3. 准备配置文件[nginx.conf]
	4. 编写docker-compose.yml文件启动集群
	web访问192.168.235.146:82，刷新刷新刷新……


Harper Cheung <harper.cheung@qq.com>
